\chapter{Revisão da Literatura}
\label{cap:3revisaoliteraria}

Neste capítulo serão apresentadas as revisões sistemáticas relacionadas
ao método de inversão acústica e aos modelos de super-resolução de imagens.
Esta revisão evidenciou o potencial de pesquisa desta proposta, pois
apresenta uma lacuna em métodos de pós-processamento da inversão sísmica.

Para esta revisão sistemática foram consultados os portais \textit{Google Scholar}, \textit{Science Direct}, \textit{IEEE Xplore}.
As buscas tiveram um alcance de dez anos e compreenderam as seguintes palavras-chaves: \textit{Deep Learning},
\textit{Convolutional Neural Network}, \textit{Super-resolution}, \textit{Super resolution}, \textit{Seismic Inversion},
\textit{Acoustic Inversion}. Com estas palavras-chaves se definiu as seguintes \textit{queries} para consulta nos periódicos:

\begin{itemize}
 \item 
 \item
 \item
 \item
\end{itemize}

Não há evidência, na literatura, de trabalhos que abordem o problema do aumento de resolução
das imagens de propriedades petrofísicas pós-inversão através de métodos de aprendizado supervisionado,
como as redes neurais convolucionais. De acordo com \cite{Xiaoyu2012}, um caminho
para melhorar a resolução da inversão sísmica é adicionar alta frequência
na aquisição e processamento do dado sísmico. Entretanto, expandir a reflexão de alta frequência é uma tarefa
difícil por conta de fatores como atenuação da terra, ruído de alta frequência, 
entre outros. Além disso, como já mencionado na seção \ref{sec:map}, o próprio modelo convolucional de inversão
limita a sísmica em uma determinada banda de frequência. Assim, a estratégia sugerida neste trabalho
objetiva a inserção de faixas de alta frequência no pós-processamento da propriedade invertida.

Embora a CNN seja um método antigo, o estudo e desenvolvimento de modelos para resolver problemas de super-resolução
é recente, cujos primeiros trabalhos datam de 2014. Os modelos que representam o estado da arte
em inversão acústica e redes convolucionais para super-resolução são discutidos nas seções a seguir.

\section{Métodos de Inversão Sísmica}
É importante ter em mente que, durante a inversão, as operações são realizadas
sobre dois espaços de representações diferentes: o espaço do modelo e o espaço de dados.
No contexto da inversão sísmica, os dados sísmicos são representados no espaço
dos dados e a propriedade de impedância acústica, por exemplo, é representada
no espaço do modelo.
A escolha dos parâmetros do modelo geralmente é não única, de modo que dois conjuntos
de parâmetros diferentes podem ser equivalentes.
Entretanto, para uma abordagem quantitativa, uma parametrização precisa ser definida \cite{tarantola} e,
no contexto da inversão acústica, o parâmetro adotado é a impedância acústica.
Para obter informações sobre os parâmetros do modelo, é necessário
realizar observações através de experimentos físicos, como por exemplo, a
aquisição sísmica.

Sob um olhar ingênuo é possível questionar por quê não definir a
função inversa da modelagem direta e calcular, de forma imediata,
os parâmetros do modelo a partir dos espaço dos dados.
No entanto, os métodos de inversão direta sofrem de instabilidades
devido ao ruído e características do problema \citep[p. 50]{sen_livro}. Outra
opção é utilizar tentativa e erro para ajustar os parâmetros até conseguir uma
resposta semelhante aos dados experimentais. Formalmente isto é automatizado
utilizando métodos de otimização. Para tanto, é preciso definir uma função de
custo, ou função objetivo, que mede o ajuste dos dados produzidos pelos
parâmetros do modelo (dado sintético) ao dado medido.

\subsection{Inversão Sísmica Linear e Não Linear}
O conteúdo desta seção apresenta as suposições de linearidade necessárias
que tornam a inversão acústica um processo analítico e computacionalmente
eficiente. O detalhamento matemático pode ser consultado em \cite{Figueiredo2014,Figueiredo17}.

Para entender o processo de inversão sísmica, é conveniente ter em mente
que os problemas inversos podem ser classificados de acordo com a natureza
do relacionamento entre os dados e o modelo, e de acordo com o comportamento da função objetivo.
Assim, eles podem ser: linear, fracamente não-linear, quasi-linear e não-linear.
Na maioria dos problemas geofísicos o operador direto $G$ é não-linear.
Como nos algoritmos de aprendizagem de máquina, na inversão sísmica
a não-linearidade implica em uma função de custo com forma complicada,
possivelmente com mínimos locais.
Por outro lado, se o operador $G$ for aproximadamente linear, a
função de erro se tornará convenientemente quadrática em relação a perturbações
no espaço do modelo. A maior parte da teoria de inversão é baseada em problemas
de inversão linear e, em muitas aplicações, ela é 
adequada para representar a natureza do sistema \cite{sen_livro}.

O modelo sísmico direto pode ser representado pelo modelo convolucional dado por:
\begin{equation}
d(t) = \int_{-\infty}^{\infty} s(\tau) r(t - \tau)\mathrm{d}\tau + e_{d}(t)
\label{eq:conmodel}
\end{equation}
onde $d(t)$ é o traço sísmico, $s(t)$ é a \textit{wavelet}, $e(t)$ é
um ruído aleatório e $r(t)$ é o refletividade.
A representação discreta para o modelo convolucional do dado sísmico é
dada pela operação matricial: 
\begin{equation}
\label{eq:sismDiscreta}
\mathbf{d = Sr + e}
\end{equation}
onde $\mathbf{S}$ é uma matriz convolucional construída utilizando uma
\textit{wavelet}, $\mathbf{r}$ é a matriz de refletividades e $e$ é um
ruído admitido. Em teoria, o ruído é uma interferência aleatória que não se tem
controle, na prática se considera ruído tudo que não é explicado pela função
$G$, e.g. imprecisões no modelo físico e problemas com filtragem e processamento
dos dados.

Como já mencionado, a relação entre o pulso sísmico e a
propriedade de impedância acústica é não-linear.
Para escapar da problemática da não-linearidade do operador direto, é
necessário aproximar linearmente o pulso sísmico da impedância acústica.
Para isto, duas medidas são necessárias. A primeira é admitir a 
refletividade como o logaritmo da impedância acústica (equação \ref{eq:lnz}).
Esta aproximação é válida para valores de refletividade menores que $0.3$.
\begin{equation}
r(t) = \frac{1}{2}\Delta \ln(z(t))
\label{eq:lnz}
\end{equation}

A segunda medida, é adotar um operador diferencial $\textbf{D}$. Assim,
se define o operador linear $\textbf{G=(1/2)SD}$ e o modelo $m=ln(z)$.
Com isto, a relação entre o dado sísmico e o parâmetro do modelo (impedância acústica)
se torna linear por:
\begin{equation}
\label{eq:sismDiscreta2}
\mathbf{d = Gm + e}
\end{equation}

Quando não é possível o uso da aproximação da Equação \ref{eq:lnz}, o problema
deve ser abordado utilizando métodos de otimização não-linear. Com isso os erros
devido às aproximações do modelo \textit{forward} diminuem, mas a otimização se
torna mais custosa. Como a relação entre os dados e os parâmetros é não linear, a
função objetivo a ser minimizada irá possuir mínimos locais, tornando necessário
o uso de métodos de otimização global. Esta prática está bem documentada na
literatura de inversão, como o uso de \textit{simulated annealing}
\citep{max_inv_simulated}, de algoritmos genéticos \citep{MallickGeneticInve} e
enxame de partículas \citep{zhe_nonlinear}. 

\subsection{Máximo \textit{a posteriori}}
\label{sec:map}

A teoria mais simples e genérica possível é obtida quando se usa uma
abordagem probabilística \citep{tarantola}. Na solução para a inversão
sísmica, os parâmetros do modelo convolucional da equação \ref{eq:sismDiscreta2}
podem ser representados em termos de suas distribuições de probabilidade.
No modelo estocástico proposto por \cite{Figueiredo2014}, as distribuições
são consideradas normais e multivariadas e são denotadas por $N(\boldsymbol{\mu},\boldsymbol{\Sigma})$.
Assumindo que o ruído $\boldsymbol{e}$ respeita uma distribuição também gaussiana,
as distribuições de probabilidade para o vetor dos dados sísmicos experimentais
$\boldsymbol{d}$, para a \textit{wavelet} $\boldsymbol{w}$ e para
o vetor dos parâmetros do modelo $\boldsymbol{m}$ são
definidos, respectivamente, pelas distribuições \ref{eq:psismico}, \ref{eq:pwavelet} e \ref{eq:pmodelo}.

\begin{equation}
\label{eq:psismico}
p(\boldsymbol{d}|\boldsymbol{\mu_{d}},\boldsymbol{\Sigma_{d}}) =
N(\boldsymbol{\mu_{d}},\boldsymbol{\Sigma_{d}})
\end{equation}
Onde $\boldsymbol{\mu_{d}} = \boldsymbol{Gm}$ é o vetor com a sísmica
sintética e $\boldsymbol{\Sigma_{d}}$ é a matriz de covariância do ruído da
sísmica, a qual é definida conforme a confiabilidade que o especialista tem no
dado sísmico ou seu nível de ruído.

\begin{equation}
\label{eq:pwavelet}
p(\boldsymbol{s}|\boldsymbol{\mu_{s}},\boldsymbol{\Sigma_{s}}) =
N(\boldsymbol{\mu_{s}},\boldsymbol{\Sigma_{s}}),
\end{equation} 
Onde o valor esperado da \textit{wavelet} $\boldsymbol{\mu_{s}}$ é definido
como um vetor nulo. Para que o método possa ser aplicado para a inversão
acústica, é necessário estimar uma \textit{wavelet} que possa ser aplicada
no modelo convolucional. Esta estimativa é realizada aplicando este mesmo processo
de inversão na região de ocorrência de amostragem de dados, um poço perfurado por exemplo, onde a refletividade pode
ser calculada diretamente \citep{Figueiredo2014}. O algoritmo de
Gibbs então é utilizado para amostrar na distribuição posterior da \textit{wavelet}, 
o valor médio e a variância são calculados.

\begin{equation}
\label{eq:pmodelo}
p(\boldsymbol{m}|\boldsymbol{\mu_{m}},\boldsymbol{\Sigma_{m}}) =
N(\boldsymbol{\mu_{m}},\boldsymbol{\Sigma_{m}}),
\end{equation} 
Na representação da distribuição dos parâmetros do modelo é possível inserir no método de inversão informações \textit{a priori}
que eventualmente estejam disponíveis. Por exemplo, $\boldsymbol{\mu_{m}}$ pode ser
uma matriz de baixas frequências gerada a partir da interpolação da impedância acústica observada em dois poços
já perfurados \citep{Figueiredo2014}.

% \begin{equation}
% \label{eq:correlVert}
% \boldsymbol{\nu}_{t,t'} = \sigma_{m}^{2} exp \left (-\frac{(t-t')^2)}{L^2}
% \right ),
% \end{equation} 
% que define a correlação entre as componentes de $\boldsymbol{m}$ no tempo $t$ e
% $t'$, na qual $\sigma_{m}^{2}$ é a variância da impedância acústica calculada
% nos dados de poços sem a baixa frequência, e $L$ é a distância de correlação
% vertical a ser imposta ao resultado.
% 
A inversão por Máximo \textit{a posteriori} (MAP)
\citep{Buland01012003,leandroGRSL} é realizada para cada traço individualmente.
As distribuições condicionais e o modelo convolucional apresentados anteriormente
são as estruturas necessárias para realizar a inversão acústica. O ponto de partida
é a aplicação do próprio método para estimar a \textit{wavelet},
com ela é possível estimar as distribuições de probabilidades envolvidas no modelo.
Em seguida, basta calcular a exponencial do modelo convolucional para obter a distribuição posterior
para o parâmetro do modelo, que no caso em questão é a impedância acústica. Esta distribuição é dada por:

\begin{equation}
p(\boldsymbol{m}|\boldsymbol{d_{o}},\boldsymbol{s},\boldsymbol{\mu_{m}},\sigma_{d}^{2},\sigma_{m}^{2}) = 
N(\boldsymbol{\mu_{m|}},\boldsymbol{\Sigma_{m|}}),
\end{equation} 

A média e variância posterior para cada traço podem ser calculadas analiticamente via \citep{leandroGRSL}:

\begin{equation}
\label{eqn:mapSolution}
\boldsymbol{\mu}_{m|} = \boldsymbol{\mu}_{m} + \boldsymbol{\Sigma}_{m}\boldsymbol{G}^{T}(\boldsymbol{G\Sigma}_{m}\boldsymbol{G}^{T}+\boldsymbol{\Sigma}_{d})^{-1}\left ( \boldsymbol{d}_{o} - \boldsymbol{G\mu}_{m} \right ),
\end{equation}
\begin{equation}
\boldsymbol{\Sigma}_{m|} = \boldsymbol{\Sigma}_{m} - \boldsymbol{\Sigma}_{m}\boldsymbol{G}^{T}(\boldsymbol{G\Sigma}_{m}\boldsymbol{G}^{T}+\boldsymbol{\Sigma}_{d})^{-1}\boldsymbol{G\Sigma}_{m}.
\end{equation} 
onde o cálculo da matriz inversa acima pode ser aproveitado para vários traços
de uma região de interesse em certos casos, ou seja, quando as matrizes de
covariância possam ser assumidas iguais para todos os traços da sísmica da região.
Desta forma alteram-se a sísmica $\mathbf{d}_0$ e a baixa frequência
$\boldsymbol{\mu_m}$ obtendo-se a média posterior para o traço desejado.
  
A matriz de covariância posterior indica a incerteza
presente no resultado, não é necessário
definir a tolerância de ajuste aos dados explicitamente, mas é preciso definir a
matriz de covariância \textit{a priori} do resultado esperado, ou seja, é
preciso ter conhecimento, mesmo que de forma grosseira, das correlações espaciais e
variâncias que se espera do resultado. Outro ponto relevante é o fato da
solução para o método de inversão MAP ser expressa em termos da covariância
e do valor esperado. Com isto as imagens de impedância acústica obtidas
se caracterizam por serem suavizadas, principalmente na região de
transição entre camadas. Durante a convolução, a \textit{wavelet} funciona
como uma modeladora, de modo que as altas frequências são filtradas
e o nível de detalhes das imagens se torna limitado. 

\section{Métodos de Super-resolução de Imagens}
Super-resolução é o processo de obter uma ou mais imagens de alta
resolução a partir de uma ou mais imagens de baixa-resolução
através do aumento no número de pixel por unidade de área.
Os algoritmos de super-resolução têm aplicação nas mais diferentes áreas
tais como, processamento de imagens aéreas e de satélite, reconhecimento de
íris, holografia digital, melhoramento de imagens faciais e de texto, entre
outras. Os modelos de super-resolução podem ser classificados
de acordo com diferentes fatores como: o domínio de aplicação,
o número de imagens de baixa resolução aplicadas e o método de reconstrução \citep{Nasrollahi2014}.

Métodos baseados em interpolação são fáceis de implementar e amplamente utilizados,
entretanto estes métodos sofrem de falta de expressividade, uma vez que modelos lineares
não são capazes de expressar dependências complexas entre as entradas e as saídas \citep{HsiehAndrews1978}.
Na prática tais métodos falham na tentativa de prever adequadamente detalhes de alta frequência
levando a saídas de alta resolução borradas. Efeito semelhante ocorre durante a inversão sísmica,
na qual as imagens resultantes apresentam resolução limitada e contornos borrados.

\subsection{Super-resolução por CNN}

Os algoritmos de super-resolução realizam buscas por
fragmentos de estruturas e os combinam para criar detalhes
de alta frequência \citep{Freeman2002,Huang2015}. Há abordagem no sentido
de melhorar os métodos de interpolação simples através da construção
de dicionários de filtros pré-treinados e selecionar os fragmentos
por algum algoritmo de \textit{Hashing} \citep{Romano2017}. Os algoritmos citados
representam o estado da arte dos métodos baseados em interpolação, cujo foco está na
velocidade de inferência. As redes convolucionais, por outro lado,
focam na construção das imagens de alta resolução, de modo a obter magnitudes
cada vez maiores de detalhes.

A aplicação de redes convolucionais para realizar super-resolução é uma abordagem
recente na literatura. Os primeiros trabalhos neste sentido datam
do ano de 2014 e visaram a super-resolução de imagens únicas no
domínio espacial \cite{dong16}. As redes convolucionais extraem implicitamente
múltiplas camadas de abstração através da otimização dos seus filtros.
Elas são capazes são capazes de modelar a distribuição conjunta sobre
uma imagem $x$ como o produto de distribuições condicionais \citep{Oord16}:
\begin{equation}
\label{eqn:prodcnn}
p(x) = \prod_{i=1}^{n^2}p(x_i|x_1,...,x_{i-1})
\end{equation}
onde, $x_i$ é o pixel modelado. A imagem é percorrida linha por linha e cada
\textit{pixel} depende apenas dos \textit{pixels} localizados em uma vizinhança pré-determinada.
Para garantir esta propriedade se define uma máscara para os filtros da convolução,
ou seja, é atribuído valo $0$ para os pesos fora da zona de interesse.
A nova imagem é gerada sequencialmente, cada pixel é reinserido para a rede para que o próximo pixel seja previsto,
deste modo cada \textit{pixel} depende fortemente dos \textit{pixels} anteriores
sob uma perspectiva não-linear.

O modelo condicional sofreu melhoria para que permitiram o reconhecimento de estruturas mais complexas.
Entre as camadas convolucionais do modelo condicionado foram adicionadas unidades
multiplicativas com a seguinte ativação:
\begin{equation}
\label{eqn:actvcnn}
y = tanh(W_{k,f} * x)\odot \sigma(W_{k,g}*x)
\end{equation}
onde $\sigma$ é a função sigmoide, $k$ é o número da camada, $*$ é o operado
convolucional e $\odot$ é o produto membro a membro das duas matrizes. Este modelo é
conhecido como \textit{Gated PixelNN} \citep{Oord16}.

O modelo de super-resolução é condicionado a um conjunto de
descrições $\boldsymbol{h}$, de modo que o modelo da distribuição condicional é dada
por:
\begin{equation}
\label{eqn:prodcnncond}
p(x|\boldsymbol{h}) = \prod_{i=1}^{n^2}p(x_i|x_1,...,x_{i-1}|\boldsymbol{h}).
\end{equation}
Desta forma, as ativações das camadas convolucionais dependem de $\boldsymbol{h}$,
antes de passarem pela função de não-linearidade. Se $\boldsymbol{h}$ contiver
informações referentes a classes de características encontradas nas imagens, todas as camadas terão um \textit{bias} que determina
a dependência destas classes. Entretanto, esta dependência não estará relacionada à
localização do \textit{pixel} na imagem. Por outro lado, se $\boldsymbol{h}$ for mapeado
para uma representação espacial $\boldsymbol{s}=m(\boldsymbol{h})$, onde $m$ é uma rede
deconvolucional, as camadas convolucionais terão \textit{biases} dependentes da localização
das estruturas contidas em $\boldsymbol{h}$, presentes na imagem. Assim, a equação \ref{eqn:actvcnn}
ganha a seguinte forma:
\begin{equation}
\label{eqn:actvcnncond}
y = tanh(W_{k,f} * x + V_{k,f}*\boldsymbol{s})\bigodot \sigma(W_{k,g}*x + V_{k,g}*\boldsymbol{s})
\end{equation}

O modelo de rede condicional proposto por \cite{DahlNS17} representa o estado da arte em modelos convolucionais para
super-resolução de múltiplas imagens. O modelo é composto de uma rede condicionante, do tipo tipo ResNet \citep{He2016}
e uma rede \textit{prior}, to tipo \textit{Gated PixelNN} \citep{Oord16}.
A rede condicionante realiza o mapeamento de uma imagem de baixa resolução para uma estrutura probabilística
de alta resolução. Assim, ela permite compor a estrutura da imagem alta resolução através da distribuição
de probabilidade marginal dos \textit{pixels} na imagem de baixa resolução. A rede \textit{prior} adiciona detalhes de alta resolução
para tornar as saída mais realísticas. 

Para treinar um modelo que mapeie uma imagem $x$ de baixa resolução em uma imagem $y$ de alta resolução,
dada uma imagem $y^*$ considerada a realidade desejada, é preciso otimizar os parâmetros
$\theta$ da distribuição condicional $p_{\theta}(\boldsymbol{y}|\boldsymbol{x})$ de modo
a maximizar a função objetivo condicional dada por \citep{DahlNS17}:
\begin{equation}
\label{eqn:objfunc}
O(\theta|\mathcal{D})= \sum_{(\boldsymbol{x},\boldsymbol{y^*})\in \mathcal{D}} log p(\boldsymbol{y^*}|\boldsymbol{y}),
\end{equation}
onde $\mathcal{D} \equiv \{(\boldsymbol{x}^{(i)},\boldsymbol{y}^{*(i)})\}_{i=1}^N$ denota o conjunto
de treinamento da rede, composto pelos pares de imagens de baixa resolução e de alta resolução que representa
a realidade observada.

Dada uma imagem $ \boldsymbol{x} \in \mathbb{R}^L $, $A_i(\boldsymbol{x}) : \mathbb{R}^L \rightarrow \mathbb{R}^K$
representa a rede condicionante capaz de prever um vetor de valores que correspondem a $K$ valores
possíveis que o $i$-ésimo \textit{pixel} de saída pode assumir. Analogamente,
$B_i(\boldsymbol{y}_{<i}) : \mathbb{R}^{i-1} \rightarrow \mathbb{R}^K$ representa a rede \textit{prior}
capaz de prever um vetor de valores do $i$-ésimo \textit{pixel}. A previsão da distribuição
sobre o $i$-ésimo \textit{pixel} de saída é obtida pela adição dos dois conjuntos de saída e aplicação
do operado de \textit{softmax}:

\begin{equation}
\label{eqn:cnnpred}
p(y_i|\boldsymbol{x},\boldsymbol{y}_{<i}) = softmax(A_i(\boldsymbol{x}) + B_i(\boldsymbol{y}_{<i}))
\end{equation}

O algoritmo Gradiente Descente Estocástico é usado para otimizar os parâmetros $A$ e $B$, a fim de maximizar
a \textit{log-likelihood} da equação \ref{eqn:objfunc}. O aprendizado da rede ocorre pela otimização da função de custo entre
as predições do modelo (equação \ref{eqn:cnnpred}) e os valores discretos da imagem que representa
a realidade $y_i^* \in \{1...K\}$:

\begin{equation}
\label{eqn:cnncostfunc}
O = \sum_{(\boldsymbol{x},\boldsymbol{y^*})\in \mathcal{D}} \sum_{i=1}^{M}\big(\zeta [\boldsymbol{y_i^*}]^T(A_i(\boldsymbol{x}) + B_i(\boldsymbol{y}_{<i}^*))
-lse(A_i(\boldsymbol{x}) + B_i(\boldsymbol{y}_{<i}^*))) \big)
\end{equation}

Mais recentemente, os avanços das pesquisas do Google em \textit{Deep Learning} disponibilizaram
ferramentas de implementação de diferentes algoritmos de aprendizagem de máquina. Dentre estas
ferramentas está o \textit{Framework} de \textit{Deep Learning} TensorFlow, no qual os modelos
de redes convolucionais podem ser implementados e testados.

\section{Resumo}

Neste capítulo foram revisados os estados da arte em inversão sísmica acústica e modelos
de rede convolucional para super-resolução.
Pontos críticos dos métodos foram considerados e
identificados para pesquisa futura. O próximo capítulo irá definir a proposta de
pesquisa, apresentar o plano de trabalho e concluir com as perspectivas de
contribuição.
