
%% bare_conf_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE Computer
%% Society conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference,compsoc]{IEEEtran}
% Some/most Computer Society conferences require the compsoc mode option,
% but others may want the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.



\usepackage{graphicx}


% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Bare Demo of IEEEtran.cls for\\ IEEE Computer Society Conferences}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Isaac Sacramento\\ and Mauro Roisenberg\\ and Rodrigo Exterkoetter}
\IEEEauthorblockA{Departament of Computer Science\\
Federal University of Santa Catarina\\
Florianopolis, Santa Catarina\\
Email: isaac.sacramento@posgrad.ufsc.br}
\and
\IEEEauthorblockN{Leando P. de Figueiredo}
\IEEEauthorblockA{Departament of Physics\\
Federal University of Santa Catarina\\
Florianopolis, Santa Catarina\\
Email: homer@thesimpsons.com}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page (and note that there is less available width in this regard for
% compsoc conferences compared to traditional conferences), use this
% alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
In this paper we will present a new convolution neural network
model to deblurr post-insversion acoustic impedance images.

\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introduction}
%Contextualização
Deblurring is the task of estimating a sharp latent image,
given a blurry image as input. Recovering the original image
is possible if the details of the blurring method are known, but
in most cases, blurry images lack of enough information
to recover a unique image \cite{}. It is not observable in the literature
an algorithm for deblurring all objects. Thus, methods that exploit
domains-specific knowledge have emerged for deblurring
categories of objects, e.g., text, faces and motion images \cite{Grigorios2017}.

Aqui, eu tentaria colocar uma frase do tipo:  
Na área da exploração petrolífera, o processo de 
aquisição sísmica e o seu processamento posterior 
de inversão para atributos de impedância acústica 
gera tipicamente imagens "borradas" da subsuperfície, 
onde não é possível observar de forma clara, as 
interfaces entre as diferentes camadas de rocha, 
estruturas geológicas, como canais, etc. 

Similarly,
the focus of this work is in post-inversion acoustic impedance deblurring.
Obtaining high resolution acoustic impedance images, through seismic inversion methods,
is a critical part in oil reservoir characterization.
Despite the notorious efficiency of the inversion methods,
post-inversion images deblurring has not received much attention.
%Specifically, acoustic impedance deblurring must take into account the fact that
%the resulting image still must to reflect the synthetic seismic data characteristics.

%O GAP
Reservoir characterization aims to determine a multidimensional
structure and properties of an oil field. To achieve this goal, it is essential to combine, through an
inversion algorithm, the informations, knowledges and available data about the field,
in such a way that it is possible to make the
quantitative predictions about the reservoir behavior \cite{buiting}.
%One of the data used in this process is the seismic amplitude.
The seismic data is widely used in inversion processes because of its facility and precision
in interpreting the acoustic impedance property.
To succeed in seismic inversion, it is necessary to include strategies to deal with multiple
sources of uncertainties. Specifically, the limited band-width of the seismic data leads to
a misinterpretation of the resulting acoustic impedance models.
According to \cite{xiaoiu}, improving the resolution of seismic inversion is
possible by adding high frequency in acquisition and processing seismic data.
However, the earth attenuation, high-frequency noise and other sources, cause
the lack of high and low frequencies in seismic data. 
Thus, deblurring the acoustic impedance models, as a post-inversion refinement process, should lead to a more accurate
interpretation of the impedance models.

Deblurring is generally modeled as the convolution of a blur kernel $k$
with a latent image $I$: 
\begin{equation}
 y = k \otimes I + n
 \label{eq:deblurr}
\end{equation}
where $n$ is the noise. Since $k$, $I$ and $n$ are unknown, the problem 
is highly ill-posed and admits infinity solutions for $k$ and $I$.
Several works have developed different deblurring methods for specific purposes.
Blind deconvolution methods are widely investigated in image processing \cite{bishop}.
For the last six years, considerable effort has been employed in single image
\cite{babacan,Krishnan2015,Levin2011,Zhang2011} and multi-image \cite{sroubek2012,Zhu2012} blind deconvolutions. 
Applying blind deconvolution generally implies in making assumptions on
blur kernels and/or on latent images. For example, assuming sparsity of blur kernel
or that natural images have super-Gaussian statistics. The second assumption
leads to the use of image priors on inference process and, consequently, to the maximum \textit{a posteriori}
(MAP) estimation \cite{babacan}. However, \cite{Levin} show that deblurring methods
based on this prior tend to favor blurry images over original latent images.
%,specially for algorithms formulated within the MAP framework.

The Bayesian inference approach \cite{Levin} outperforms the MAP based methods. It marginalizes
the image from the optimization step, while estimating the unknown blur.
The authors show that it is possible to define a class of prior image
based on natural image statistics, suitable enough to represent sharp images features.
This prior formulation makes possible to use Bayesian inference in the estimation of the
unknown image and the blur kernel. According to \cite{Hacohen13}, defining a gradient
prior, by itself, is not sufficient to reach a sharp image, instead,
they search in a dataset for a prior that densely correspond to
the blurry image similar to a sharp image. This search is an
iteratively optimization over the correspondence between the images, the kernel and
the sharp image estimation. Although \cite{Pan2014} suggest a generalization
for the method proposed by \cite{Hacohen13}, it still requires a similar reference image,
which is not always available.

The optimization methods previously described use a set of priors based on
generic image statistics or domain-specific priors. It has been demonstrated
that these methods work properly on synthetic blurs. However, newly studies show that they failure
when applied to real world blurry images \cite{Lai2016} and take a severe computational cost \cite{Chakrabarti2016}.
In contrast, the learning-based methods have gained attention with the resumption and recent advances in neural
networks. The adequate hyper-parameter adjustment allows neural network to learn
non-linear function or blur kernels. Thus, deblurring becomes a function of a blurry image $I$
and a set of parameters $p$ as
\begin{equation}
 y = \sigma(I,p)
 \label{eq:deblur}
\end{equation}
Learning-based methods focus on developing a model to learn the function $\sigma$ \cite{Hradis2015}
and to perform non-blind deblurring \cite{Chakrabarti2016}. \cite{Sun2015} learn a convolution neural networks (CNN) to
recognize motion kernels and performs non-blind deconvolution in
dense motion field estimate, in addition, \cite{Hradis2015} minimize regularized $l_2$ 
in order to perform text deblurring.
% , mainly because 
% of the absent of high-frequency details \cite{Levin}.
%However, the deep the investigated horizon is, the big the limitation in using the seismic amplitude \cite{riel}.
%As a consequence of this limitation, the post-inversion acoustic impedance 
%The seismic data inversion has proved to be an efficient tool to accurately integrate the seismic information
%in order to generate models that contribute to an effective reservoir characterization \cite{sergio}.
%According to \cite{Latimer}, a good acoustic impedance model contains more information
%then the seismic data because this model keeps all the seismic data information and additional
%information originated in the well-logs.
% Depending on the applied method, the acoustic impedance volume is the result of
% the integration of data from different sources, such as, the seismic data, well logs and
% velocity models. Thus, building an acoustic impedance model is the natural process to 
% integrate the available informations to obtain a model accessible to geophysicists, geologists
% and engineers. 
% The acoustic impedance models allow fast interpretations and an efficient identification of
% exploratory targets in seismic scale. 
We approach  the acoustic impedance deblurring through
a CNN model. CNN is a framework of deep learning which has been
used in a wide sort of machine learning tasks. The availability of benchmarks
\cite{Russakovsky} and the advances in Graphical Processing Unit (GPU) \cite{Buduma15}
allowed CNN to outperform state-of-the-art techniques in detection \cite{Girshick,Bell}, model-free
tracking \cite{Nam}, classification \cite{He}. With excellence in feature learning,
CNN achieved notorious success in image and video classification \cite{Krizhevsky, AbdelHamid}, action  and speech recognition \cite{Farfade, S_Ji}.
Under the perspective of the reservoir characterization, CNN has been applied to lithofacies recognition
\cite{qian} and calculation \cite{Liu}. However, there is a lack of researches on improving the resolution of
images resulting from inversion processes.

%A solução
In this paper, we propose a new multilayer convolution network model to perform deblurring in post-inversion acoustic impedance.
Each network layer maps higher level features originated in the previews layers through convolutional blur kernels.
To perform this mapping, the kernels, also named weights, are adjusted by minimizing a loss function. 
The model enhances the resolution of acoustic impedance images, resulting in sharp images with
increased high-frequency band-width and lower noise. 
In order to train the model, we blur a set of the synthetic acoustic impedance images to create a dictionary of
images of high and low resolution. Then, the pairs of blur and latent images are normalized and 
presented to the network as input and target, respectively.
%We compare the deblurred images depicted by our model and by two other deblurring methods ().
The core concept of our architecture is the combination of the convolution layers with regression
layers, thus the convolutional layers learn the spatial structures existing in different
acoustic impedance images, while the regression layer proceed the prediction of the property values.
%\hfill mds
 
%\hfill August 26, 2015

\section{Theoretical foundations}
Inversion theory is used in several areas to infer parameters values
related to physical processes based on experimental data.
Inversion modeling refers to using the current measurements of observable
physical parameters in order to infer the current model parameters (not observable).
The inversion problem is described as (Eq. \ref{eq:deqgm})
\begin{equation}
\label{eq:deqgm}
m = F^{-1}(d)
\end{equation}
where, $F$ is the investigated physical system, and relates a set of model parameters
$m=(m_1, m_2,...,m_n)\subset R^n$ estimated through the observed data $d \in R^s$.
Geophysical methods frequently involve the solution and assessment of inversion problems.
Studying these problems allow inferring physical properties distributions in the earth subsurface, using measured
data. Among these data, the seismic data is mainly used in seismic inversion, which plays an important role in
reservoir characterization. From a practical perspective, solving seismic inversion problems improves
the exploration and management in oil industry, once the seismic data is highly correlated to petrophysical
properties, e.g., density and porosity in subsurface.

The offshore seismic data is the main observable data used in seismic inversion. To perform seismic acquisition,
one sends pulses through a controlled artificial source and captures
the vertical component responses in function of time. The seismic data is a composition of
the wave pulse used in the acquisition, named wavelet, and the characteristic of the interfaces between rock layers,
on which the wavelet reflects. This rock layer characteristic is called reflectivity coefficient and it is
calculated as
\begin{equation}
r(t) = \frac{z(t+\delta t)-z(t)}{z(t+\delta t)+z(t)},
\label{eq:refletv}
\end{equation}
where, $z(t)$ is the acoustic impedance, in function of time $t$, defined as 
$z(t)=\rho(t)v(t)$, where $\rho(t)$ is the rock density and $v(t)$ the propagation velocity
of acoustic wave.
Therefore, the seismic data  $d(t)$ is modeled as a discrete convolution operation $*$ of the wavelet $s$ with the
reflectivity coefficient $r$ as
\begin{equation}
d(t) = s(\tau) * \sum_{j-1}^{N}{r(t- t_j) \delta(t - t_j) + e_d(t)}
\end{equation}
where, $N$ is the number of subsurface layers, $e_d(t)$ is a random noise in function of time.
One ideal wavelet is a delta pulse with all the frequency band-width, however, in practice
wavelets have their band-width generally limited from $6Hz$ to $65Hz$. By consequence,
the images resulting from the seismic inversion will keep their frequency spectrum limited.

According to \cite{Latimer}, a good acoustic impedance model contains more information
than the seismic data, because the inversion process contains additional information originated from well-logs, for
exemple, a low-frequency model.
The well-logs are real data measured in wells spread along the field.
With the local acoustic impedance it is possible to calculate the low-frequency
model by interpolation between wells \cite{Buland2003,Figueiredo2012}. Despite of the
low-frequency model, the final model for acoustic impedance still lacks of high resolution details.

\section{Data and Methods}
\subsection{Acoustic Impedance Inversion}
The experiments described in this paper perform
on a set of synthetic acoustic impedance images. Using synthetic models
to test and parametrize algorithms is a common practice in reservoir characterization \cite{sergio}. It allows studying the
results of the algorithms without external interferences and performing efficient interpretations and assessments. 
According to \cite{Harvey}, wedge shaped models is a straight way to analysis the
seismic model and inversion processes. It reproduces
reservoir contexts such as stratigraphic refinements, edges and channel structures in a realistic manner.

The training set generation occurs in two steps.
The fist step creates a set of wedge shapes represented
by images with size $32$ x $32$. The wedge shapes represent the reservoir and they
randomly vary in width and length. The second step fills the lithology with values of petrophysical properties.
In order to simplify the assessments and conclusions, we fill the lithology structures with constant
reference values of rock densities and compressional velocity observed in the literature \cite{Mavko2009}.
The acoustic impedance is calculated using the density and velocity models and the images in high
resolution model are obtained, as illustrated in  Fig. \ref{fig_lithology}.
\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{Figs/Image_Paper}
\DeclareGraphicsExtensions.
\caption{Wedge with reference values for density and compressional velocity.}
\label{fig_lithology}
\end{figure}
In a real scenario, the blurry acoustic impedance is the
result of an acoustic inversion method, such as Maximum \textit{a posteriori} \cite{Buland2003,Figueiredo2012}, Sparse Spike \cite{Debeye1990} 
and Recursive Inversion \cite{Chopra2001}, using seismic data with limited band-with.
However, for experimental purpose, the acoustic impedance models were filtered
and the high frequencies were removed to obtain the blurry images, as illustrated in Fig. \ref{fig_blur}.
This way, the supervised learning is performed with the high resolution images and blurred images.
To increase the number of examples in the training dataset we rotate
the impedance models in angles integer multiples of right angle. 
This approach allows the model learning an wide edge variabilities
in wedges images.
\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{Figs/Image_Paper_blurred}
\DeclareGraphicsExtensions.
\caption{Acoustic impedance blurred model.}
\label{fig_blur}
\end{figure}

\subsection{Proposed Architecture}
The workflow of the proposed method consists of the following
steps:
\begin{itemize}
 \item Generate the synthetic impedance images;
 \item Blur the images through a low-pass filter;
 \item Train the convolutional model with the pair of high and low resolution images;
 \item Test the model with different blurry images;
 \item Assess the result for the testing output.
\end{itemize}

The CNN is a well established method for
pattern recognition and image classification.
Thus, an important breakthrough when applying CNN to
deal with physical property images is developing a model
capable to solve regression tasks. The model presented in this paper
is able to solve two important problems related to deblurring  images of physical properties
: ($1$) learning the spacial patterns in the low resolution
training images and ($2$) predicting each pixel intensity value in the new
higher resolution image.
To reach these two goals, our model, outlined in Fig. \ref{}, comprises two major
components that are combined to perform deblurring and physical property prediction
jointly: (1) convolutional component and (2) regression component. The convolutional component
is a two layer structure that maps a blurry image to a non-blurred model, while 
the regression layer is supposed to predict the continuous values of each pixel
representing the acoustic impedance value. 
The state-of-the-art CNN models for image deblurring \cite{Grigorios2017} and super-resolution \cite{Dahl2017}
work classifying each pixel in the input image for one or three channels. 
Using these methods to solve the issue of deblurring continuous physical properties 
implies in discretization of each pixel in order to generate an image file. However, returning
to the original data, results in information loss, what represents a serious inconvenient.
Thus, combining the convolutional approach with the regression layer to deblur and predict
the physics property represents a relevant advance.

\subsection{Implementation Details}
The model is implemented using the Deep Learning toolbox
delivered in MATLAB R2017A. For training the network, we use a mini-batch with size of $32$ training examples.
The network weights initialize randomly and the Stochastic Gradient Descendant with
Momentum (SGDM) \cite{Ning} is the optimization algorithm.
We adopt an exponentially decreasing learning rate (initially
set to $0.005$), decreasing every iteration in a total of $100$ iterations.
It should be noted that, once the wedge shapes are randomly generated, every image is different
and each one is introduced only once to the network, this way avoiding over-fitting.
%Falar que foram usadas 4 figuras, 

In our model, each convolutional layer is configured with $50$ kernels, sized $5$ x $5$
and stride $1$, meaning that after each convolution operation the blur kernel is shifted
one position in each direction. After each convolutional layer we proceed a maximum pooling operation
to obtain the maximum value of small regions in the input image and obtain  statistical summaries of these regions . We also
apply rectified linear units (ReLU) after each pooling layer in order to speed up the training
step and to learn sensible features from the images, following the proposal of \cite{Nair2010}. After
the second regularization layer,  we add a fully connected layer, which maps the convolution layer's output
to $1024$ neurons. Finally, the output layer comprises a regression unit to predict the intensity of acoustic impedance in each pixel.
The output vector is then resized to the original image dimension.

The model performs a supervised learning through a dictionary containing pairs of low resolution and high resolution images.
The optimization algorithm adjusts the network weights in every layer by minimizing the Mean Squared Error (MSE)
in each batch of images. Thus, after the training phase, the model is capable
to deblur any other wedge shaped acoustic impedance image not presented in the training dataset. The outputted
image recovers the high frequencies and presents higher similarity
to the high resolution image than to the blurred image, according to a established metric.
The model was adjusted to deblur a wide variety of wedge shapes and impedance values.
Those wedges which the model was unable to predict were added in the training set.

Three metrics assess the performance of the convolution network: Fast Fourier Transform Index - FFTI (Eq. \ref{eq:fourier}),
Rooted Mean Square Error - RMSE (Eq. \ref{eq:mse}). 
The FFTI is a similarity metric calculated based on the fast fourier transform (FFT) of each image.
It is introduced by \cite{naranyana} and calculates as 
\begin{equation}
 C = \frac{ (\sum_{i=1}^{N}{F_{1i}F_{2i}} - N \bar{F}_1\bar{F}_2 )^2 }{ (\sum_{i=1}^{N}{|F_{1i}|^2} - N{\bar{F}^2}_1)( \sum_{i=1}^{N}{|F_{2i}|^2} - N{\bar{F}^2}_2 )},
 \label{eq:fourier}
\end{equation}
where, for each frequency, an intensity value is calculated from the real and complex parts of the fourier
transform. $F_{1i}$ represents the intensity value of $i$-th \textit{pixel} in the first image and $F_{2i}$
is the intensity value of $i$-th \textit{pixel} in the second image. $\bar{F}_1$ e $\bar{F}_2$ are the mean
frequencies in each image. The closer FFTI is to $1$, the higher the similarity between the images.
The frequencies spectrum is additionally useful to present the graphic of frequency magnitudes in the images.
The frequency magnitude graphic allows distinguishing what high frequencies were added to the acoustic impedance
after the low resolution image is passed through the trained CNN. 
\begin{equation}
 RMSE = \sqrt{\frac{ (\sum_{i=1}^{N}{(x_i -y_i)^2 } }{N}},
 \label{eq:mse}
\end{equation}

\section{Experiments}
To build the training dataset we generate $500$ images of random wedge.
Because the last layer of the network is a regression unit, it is necessary the
normalization of the images to values between $0$ and $1$, and the results
are presented in terms of this normalization.
The normalized images are then rotated in four integer angles multiple of $90$
degrees, then comprising a total of $2000$ images. By doing so, we present to
the network the same lithologies in different positions and expect that the network
identify general sorts of wedges in angles different from those with which the it was trained.
The rotated images are blurred by applying a low-pass filter with
cutting frequency $4Hz$, then the pairs of blurred and not-blurred images are used to
adjust the model weights. It is relevant to mention that the images which are blurred with
the same cutting frequency and that remains symmetric after the rotation,
have the values for the metrics RMSE and FFTI. This means that rotating the images causes no changes in impedance values.
We apply the same approach to generate different scenarios for test cases.
In the following subsections we present the results divided into those cases in which the wedges are
rotated in angles integer multiple of right angle and those in which the wedges are rotated according
to a random angle.

\subsection{Integer Angles Rotated Wedges} \label{IARW}
We firstly test if the network is capable to correctly model the shape of the wedges
and to deblur the edges and contours in a simple perspective.
Thus, the training images and the test images have the same reference values for density and compressional velocity,
they are different only by their shapes.
\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{Figs/Caso1}
\DeclareGraphicsExtensions.
\caption{Acoustic impedance blurred model.}
\label{fig_scenario1}
\end{figure}
Comparing the blurry image and the CNN output,
in the second and third columns of Fig. \ref{fig_scenario1} respectively,
it is notable that the CNN output reached a sharp image and accurately
predicted the pixel values. The frequency magnitude graphs in the fourth
column shows that the the model precisely recovered the range of frequency
of the original image. It is noticeable in Tab. \ref{table_caso_1}
that the RMSE for the network output is less than of the blurry image,
while the FFTI.
%When we compare our model to....
\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{An Example of a Table}
\label{table_caso_1}
\centering
\begin{tabular}{|c||c||c|}
\hline
 & Blurry Image & CNN Output \\
\hline
Example 1 & $0.1294$ & $0.0644$\\
\hline
	  & $0.9819$ & $0.9857$\\
\hline
Example 2 & $0.1948$ & $0.0774$ \\
\hline
	  & $0.9549$ & $0.9862$\\
\hline
Example 3 & $0.1294$ & $0.0645$\\
\hline
	  & $0.9819$ & $0.9876$\\
\hline
Example 4 & $0.1948$ & $0.0775$\\
\hline
	  & $0.9575$ & $0.9902$\\
\hline
\end{tabular}
\end{table}

In the second scenario, we invert the impedance values of the lithology structures, that means,
we fill the reservoir lithology with normalized impedance value equal $1$ and the lithology
around of the reservoir with value equal to $0$.
%This allows to verify the disconnection between the prediction of the pixel value and the lithology shape.
Even though the network outputs show lower RMSE and recover all the frequency spectrum (Fig. \ref{fig_scenario6}), the similarity
between the CNN images is lower than the blurred images.
This result is related to the fact that the model reconstruct the lithology structures, however they lack of thin
details, what makes them slightly less similar to the latent image.
\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{Figs/Caso6}
\DeclareGraphicsExtensions.
\caption{Acoustic impedance blurred with inverted property values.}
\label{fig_scenario6}
\end{figure}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{An Example of a Table}
\label{table_caso_2}
\centering
\begin{tabular}{|c||c||c|}
\hline
 & Blurry Image & CNN Output \\
\hline
Example 1 & $0.1248$ & $0.0702$\\
\hline
	  & $0.9005$ & $0.7904$\\
\hline
Example 2 & $0.1738$ & $0.0855$ \\
\hline
	  & $0.8064$ & $0.7918$\\
\hline
Example 3 & $0.1248$ & $0.0727$\\
\hline
	  & $0.9005$ & $0.8190$\\
\hline
Example 4 & $0.1738$ & $0.0858$\\
\hline
	  & $0.8512$ & $0.7906$\\
\hline
\end{tabular}
\end{table}

In the following scenario, we arbitrarily change the normalized impedance in both
lithologies to values $ 0.7 $ out of the wedge and $ 0.3$ into the wedge.
These values are different from those learned by the model during the
training phase and we test the model generalization capacity to reach
the learned pixel intensity values. Once the model is trained with pixel valued to
$ 0 $ and $ 1 $, it poorly extrapolated and a new training dataset is generated
containing wedges with the new pixel values. It is noticeable in
Fig. \ref{fig_scenario2} that the network, trained with the new dataset, learned
the wedges shapes and the predicted pixel intensities with low variance. The values
for each image example in Tab. \ref{table_caso_3} show a short decreasing in the RMSE,
while the FTTI indicates a slight increase in CNN images similarity with the latent image.
Although the short positive change in the metrics, the output images present increasing
frequency magnitude in the range between $ 20 $ and $ 50 Hz $ and between $ 60 $ and $ 100 Hz $.
\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{Figs/Caso2}
\DeclareGraphicsExtensions.
\caption{Acoustic impedance normalized to arbitrary values.}
\label{fig_scenario2}
\end{figure}

%Explicar os resultados dos indices para este cenário
\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{An Example of a Table}
\label{table_caso_3}
\centering
\begin{tabular}{|c||c||c|}
\hline
 & Blurry Image & CNN Output \\
\hline
Example 1 & $0.3529$ & $0.0333$\\
\hline
	  & $0.9901$ & $0.9915$\\
\hline
Example 2 & $0.0561$ & $0.0444$ \\
\hline
	  & $0.9744$ & $0.9868$\\
\hline
Example 3 & $0.3529$ & $0.0319$\\
\hline
	  & $0.9901$ & $0.9908$\\
\hline
Example 4 & $0.0561$ & $0.0435$\\
\hline
	  & $0.9746$ & $0.9860$\\
\hline
\end{tabular}
\end{table}

In the last scenario, we blurr each synthetic image with a different
cutting frequency randomly chosen between $3$ and $20 Hz$. This is particularly interesting, because
in a scenario with real data, physical aspects as signal attenuation caused by the depth lead to
different blurring profiles in post-inversion images.
Fig. \ref{fig_scenario5} shows the results for this scenarios.
\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{Figs/Caso5}
\DeclareGraphicsExtensions.
\caption{Acoustic impedance blurred with different frequencies.}
\label{fig_scenario5}
\end{figure}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{An Example of a Table}
\label{table_caso_5}
\centering
\begin{tabular}{|c||c||c|}
\hline
 & Blurry Image & CNN Output \\
\hline
Example 1 ($18Hz$) & $0.0933$ & $0.0841$\\
\hline
	  & $0.9576$ & $0.8094$\\
\hline
Example 2 ($14Hz$)& $0.1277$ & $0.0983$ \\
\hline
	  & $0.9169$ & $0.8001$\\
\hline
Example 3 ($4Hz$)& $0.1228$ & $0.0795$\\
\hline
	  & $0.9228$ & $0.8299$\\
\hline
Example 4 ($8Hz$)& $0.8946$ & $0.7983$\\
\hline
  & $0.0933$ & $0.0841$\\
\hline
\end{tabular}
\end{table}

\subsection{Randomly Rotated Wedges}
The randomly rotated wedges are related to lithologies positioned at random angles.
In this case, we evaluate the CNN capability to deblur the wedges with specific shape and
position, that are absent of the training dataset.
We test the wedges with the acoustic impedance normalized to values equals to $0$ and $1$.
It is observable in Fig. \ref{fig_scenario4} that the model presents higher uncertainty
in modeling the edges and corners of the wedges. On the other side, as the test images contain the same range of values
as in the training images, the model keep its predictive capacity and recover the  This result is evident in the metrics values
(Tab. \ref{table_caso_6}). In examples 2 and 4, the network outputs present lower RMSE than the blurry images, 
while it is higher in the other examples 1 and 3. However, according to the FFTI, all the network outputs present
less similarity to the latent image, than the blurred image and we believe that is explained by the higher uncertainty
observed. A mitigation for this problem is adding examples of this image to the network dataset, as stated previously in
Section \ref{IARW}.
\begin{figure}[!t]
\centering
\includegraphics[width=3.0in]{Figs/Caso4}
\DeclareGraphicsExtensions.
\caption{Acoustic impedance blurred model.}
\label{fig_scenario4}
\end{figure}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Table of metric values for wedges in random positions and acoustic impedance normalized to $0$ and $1$.}
\label{table_caso_6}
\centering
\begin{tabular}{|c||c||c|}
\hline
 & Blurry Image & CNN Output \\
\hline
Example 1 & $0.2155$ & $0.2164$\\
\hline
	  & $0.9357$ & $0.9361$\\
\hline
Example 2 & $0.2248$ & $0.2045$ \\
\hline
	  & $0.9357$ & $0.9235$\\
\hline
Example 3 & $0.2197$ & $0.2209$\\
\hline
	  & $0.9406$ & $0.9236$\\
\hline
Example 4 & $0.2174$ & $0.1879$\\
\hline
	  & $0.9555$ & $0.9549$\\
\hline
\end{tabular}
\end{table}

% In the following scenario of random rotated wedges, we normalize the values of impedance to $0.3$ and $0.7$, as
% illustrated in Fig. \ref{fig_scenario3}. 
% 
% \begin{figure}[!t]
% \centering
% \includegraphics[width=3.0in]{Figs/Caso3}
% \DeclareGraphicsExtensions.
% \caption{Acoustic impedance blurred model.}
% \label{fig_scenario3}
% \end{figure}
% 
% Tab. \ref{table_caso_7} presents the values error values for this test case. It is notable
% that.
% \begin{table}[!t]
% \renewcommand{\arraystretch}{1.3}
% \caption{Table of metric values for wedges in random positions and acoustic impedance normalized to $0.3$ and $0.7$.}
% \label{table_caso_7}
% \centering
% \begin{tabular}{|c||c||c|}
% \hline
%  & Blurry Image & CNN Output \\
% \hline
% Example 1 & $0.0747$ & $0.0885$\\
% \hline
% 	  & $0.9390$ & $0.9180$\\
% \hline
% Example 2 & $0.07014$ & $0.0933$ \\
% \hline
% 	  & $0.9437$ & $0.8967$\\
% \hline
% Example 3 & $0.0712$ & $0.1046$\\
% \hline
% 	  & $0.9289$ & $0.8733$\\
% \hline
% Example 4 & $0.0685$ & $0.0751$\\
% \hline
% 	  & $0.9542$ & $0.9266$\\
% \hline
% \end{tabular}
% \end{table}

% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.
\section{Conclusion}
The conclusion goes here.



% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors would like to thank...

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}
\bibitem{xiaoiu}  		Xi Xiaoyu, Ling Yun, Sun Desheng, Guo Xiangyu, and Wang Huifeng, ``Studying the effect of expanding low or high frequency on post-stack seismic inversion,'' in SEG Technical Program Expanded Abstracts 2012. September 2012, 1-5.
\bibitem{Buduma15}  		Buduma, N., `` Fundamentals of Deep Learning,'' Academic Press, 2015, in O'Reilly Media.
\bibitem{qian} 			Qian Feng, Yin Miao, Su Ming-Jun, Wang Yaojun, Hu Guangmin, ``Seismic facies recognition based on prestack data using deep convolutional autoencoder,''.
\bibitem{Liu} 			Liu Lihui, Lu Rong, Li Jianhai, Yang Wenkui, ``Seismic Lithofacies Computation Method Based on Deep Learning,'' p. 649-652.
\bibitem{Chrysos} 		G. G. Chrysos, S. Zafeiriou, "Deep Face Deblurring," 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Honolulu, HI, 2017, pp. 2015-2024.
\bibitem{Latimer} 		Rebecca Buxton Latimer, Rick Davidson, Paul van Riel, ``An interpreter's guide to understanding and working with seismic-derived acoustic impedance data,'',2017, pp. 242-256, v. 19, num. 3, in The Leading Edge.
\bibitem{buiting} 		JJM. Buiting, M. Bacon,``Using geophysical, geological, and petrophysical data to characterize reservoirs in the North Sea.'', in 5th Conference on Petroleum Geology of NW Europe, London. CD-ROM.
\bibitem{riel} 			Paul van Riel,  ``The past, present and future of quantitative reservoir characterization.'', in The Leading Edge, 19, p. 878–881.
\bibitem{sergio}		Sergio Sacani Sancevero, Armando Zaupa Remacre, Rodrigo de Souza Portugal, ``O papel da inversão para a impedância no processo de caracterização sísmica de reservatórios'',in Revista Brasileira de Geofísica, 2006, p. 495-512, v. 24.
\bibitem{Russakovsky}		O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, ``Imagenet large scale visual recognition challenge,'' in ternational Journal of Computer Vision (IJCV), p. 211–252, 2015.
\bibitem{Girshick}		R. Girshick, ``Fast r-cnn,'' In IEEE Proceedings of International Conference on Computer Vision (ICCV), pages 1440–1448, 2015.
\bibitem{Bell} 			S. Bell, C. L. Zitnick, K. Bala, and R. Girshick, ``Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks,'' in arXiv preprint arXiv:1512.04143, 2015.s
\bibitem{Nam}			H. Nam and B. Han, ``Learning multi-domain convolutional neural networks for visual tracking,'' In IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2016.
\bibitem{He}			K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for image recognition,'' In IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2016.
\bibitem{Levin}			A. Levin, Y. Weiss, F. Durand, and W. T. Freeman. ``Understanding and evaluating blind deconvolution algorithms.'' In IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), p. 1964–1971.
\bibitem{Levin2011}		A. Levin, Y. Weiss, F. Durand, and W. T. Freeman. ``Efficient marginal likelihood optimization in blind deconvolution.'' In CVPR, 2011.
\bibitem{Krizhevsky}		A. Krizhevsky, I. Sutskever, G. E. Hinton, ``Imagenet classification with deep convolutional neural networks: Advances in neural information processing systems,'' 2012, p. 1097–1105.
\bibitem{S_Ji}			S. Ji,W. Xu, M. Yang, K. Yu, 2013, ``3d convolutional neural networks for human action recognition,'' in IEEE transactions on pattern analysis and machine intelligence, n. 35, p. 221–231.
\bibitem{AbdelHamid}		O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, D. Yu, 2014, ``Convolutional neural networks for speech recognition,'' in IEEE/ACM Transactions on audio, speech, and language processing, n. 22, p. 1533–1545.
\bibitem{Farfade}		S. S. Farfade, M. J. Saberian, L.-J. Li, 2015, ``Multi-view face detection using deep convolutional neural networks,'' in Proceedings of the 5th ACM on International Conference on Multimedia Retrieval, ACM, p. 643–650.
\bibitem{Harvey} 		P. J. Harvey, D. J. MacDONALD, ``Seismic modelling of porosity within the jurassic aged carbonate bank, offshore Nova Scotia,'' in  Canadian Journal of Exploration Geophysics, n. 26, p. 54–71.
\bibitem{naranyana}		S. Narayana, P. K. Thirivikraman, ``Image similarity using fourier transform,'' in International Journal of Computer Engeneering and Technology, 2015, n. 6, p. 29–37.
\bibitem{Ning}			N. Qian, ``On the momentum term in gradient descent learning algorithms,'' in Neural Networks, v. 12, i. 1, 1999, p. 145-151.
\bibitem{bishop}		T.E. Bishop, S.D. Babacan, Amizic, T. Chan, R. Molina, A. Katsaggelos, ``Blind image deconvolution: problem formulation and existing approaches.'' in. Blindimage deconvolution: theory and applications,  CRC press (2007).
\bibitem{babacan}		S. D. Babacan, R. Molina, M. N. Do, and A. K. Katsaggelos, ``Bayesian blind deconvolution with general sparse image priors.'' In. Proceedings of European Conference on Computer Vision (ECCV), p. 341–355, 2012.
\bibitem{Hacohen13}		Y. Hacohen, E. Shechtman, and D. Lischinski, ``Deblurring by example using dense correspondence.'' In IEEE Proceedings of International Conference on Computer Vision (ICCV), p. 2384–2391, 2013.
\bibitem{Pan2014}		J. Pan, Z. Hu, Z. Su, M. H. Yang., ``Deblurring face images with exemplars.'' In Proceedings of European Conference on Computer Vision (ECCV), p. 47–62. Springer, 2014.
\bibitem{Lai2016}		W.S. Lai, J. B. Huang, Z. Hu, N. Ahuja, M. H. Yang.``A comparative study for single image blind deblurring.'' In IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2016.
\bibitem{Chakrabarti2016}	A. Chakrabarti. ``A neural approach to blind motion deblurring.'' In Proceedings of European Conference on Computer Vision (ECCV), p. 221–235. Springer, 2016.
\bibitem{Hradis2015}		M. Hradiš, J. Kotera, P. Zemcı́k, F. Šroubek. ``Convolutional neural networks for direct text deblurring.'' In Proceedings of British Machine Vision Conference (BMVC), 2015.
\bibitem{Sun2015}		J. Sun, W. Cao, Z. Xu, J. Ponce. ``Learning a convolutional neural network for non-uniform motion blur removal.'' In IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), p. 769–777, 2015.
\bibitem{Krishnan2015}		D. Krishnan, T. Tay, R. Fergus. ``Blind deconvolution using anormalized sparsity measure.'' In CVPR, 2011.
\bibitem{Zhang2011}		H. Zhang, J. Yang, Y. Zhang, N. M. Nasrabadi, T. S. Huang. ``Close the loop: Joint blind image restoration and recognition with sparse representation prior.'' In ICCV, 2011. 
\bibitem{sroubek2012}		F. Šroubek and P. Milanfar. ``Robust multichannel blind deconvolution via fast alternating minimization.'' in IEEE Trans. on Image Processing, p. 1687–1700, 2012.
\bibitem{Zhu2012}		X. Zhu, F. Šroubek, P. Milanfar. ``Deconvolving PSFs for a better motion deblurring using multiple images.'' In ECCV, 2012.
\bibitem{Figueiredo2012}	L. P. Figueiredo, M. Santos, M. Roisenberg, G. Neto, W. Figueiredo, ``Bayesian framework to wavelet estimation and linearized acoustic inversion,'' In Geoscience and Remote Sensing Letters, 2012, p. 1–5.
\bibitem{Buland2003}		A. Buland,  H. Omre, ``Bayesian linearized avo inversion,'' In Geophysics, 2003, p. 185–198.
\bibitem{Mavko2009}		G. Mavko, T. Mukerji, J. Dvorkin, ``The Rock Physics Handbook: Tools for Seismic Analysis of Porous Media.'' 2009 Cambridge: Cambridge University Press, p. 359-369.
\bibitem{Debeye1990}		H. DEBEYE, P. RIEL van, ``Lp-norm deconvolution.'' 1990, Geophysical Prospecting, p. 381–403
\bibitem{Chopra2001}		S. Chopra, ``Integrating coherence cube imaging and seismic inversion.'' 2001, The Leading Edge, p. 354–362.
\bibitem{Nair2010} 		V. Nair, G. E. Hinton. ``Rectified linear units improve restricted boltzmann machines.'' In Proc. 27th International Conference on Machine Learning, 2010.
\bibitem{Dahl2017}		R. Dahl, M. Norouzi, J. Shlens, ``Pixel recursive super resolution'', 2017, CoRR.
\bibitem{Grigorios2017}		G. G. Chrysos, S. Zafeiriou, ``Deep Face Deblurring.'', 2017, p. 2015-2024, 10.1109/CVPRW.2017.252. 
\end{thebibliography}

%0 Journal Article
%A Rebecca Buxton Latimer
%A Rick Davidson
%A Paul van Riel
%T An interpreter's guide to understanding and working with seismic-derived acoustic impedance data
%D 2000
%J The Leading Edge
%P 242-256
%V 19
%N 3
%R 10.1190/1.1438580
%U http://library.seg.org/doi/abs/10.1190/1.1438580
%K 


% that's all folks
\end{document}


