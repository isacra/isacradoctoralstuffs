import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import gzip
import os

from numpy import dtype

train_dir = '/home/malik/workspace/Tensor_Basics/train'
IMAGE_HIGHT = 1
IMAGE_WIDTH = 10
NUM_CHANNELS = 1
WORK_DIRECTORY = '/home/malik/workspace/Tensor_Basics/Data'
PIXEL_DEPTH = 255
BATCH_SIZE = 1
record_bytes = (IMAGE_HIGHT*IMAGE_WIDTH*NUM_CHANNELS)

def extract_data(filename, num_images):
    if not tf.gfile.Exists(filename):
      raise ValueError('Failed to find file: ' + filename)
  
    images = np.genfromtxt(filename,delimiter=',',dtype=np.float32)
    images = images.reshape(num_images, IMAGE_HIGHT, IMAGE_WIDTH, NUM_CHANNELS)
    return images

def extract_labels(filename, num_images):
    if not tf.gfile.Exists(filename):
      raise ValueError('Failed to find file: ' + filename)
  
    labels = np.genfromtxt(filename,delimiter=',',dtype=np.float32)

    return labels

def get_file_path(filename):
    filepath = os.path.join(WORK_DIRECTORY, filename)
    return filepath

if tf.gfile.Exists(train_dir):
    tf.gfile.DeleteRecursively(train_dir)
tf.gfile.MakeDirs(train_dir)

train_data_filename = get_file_path('input_seno_data.csv')
train_labels_filename = get_file_path('output_seno_data.csv')

train_data = extract_data(train_data_filename, 90)
train_labels = extract_labels(train_labels_filename, 90)
  
plt.ion()
n_observations = 200
#fig, ax = plt.subplots(1, 1)
xs = np.linspace(-2*np.pi, 2*np.pi, n_observations)

ys = np.sin(xs) + np.random.uniform(-0.1, 0.1, n_observations)

#ax.scatter(xs, ys)

#fig.show()
#plt.draw()

xs = xs.reshape(200,1)
ys = ys.reshape(200,1)


#Placeholders, estruturas do tensorflow onde sao guardados os dados
with tf.name_scope('Inputs'):
    X=tf.placeholder(tf.float32,shape=(BATCH_SIZE, IMAGE_HIGHT, IMAGE_WIDTH, NUM_CHANNELS),name='InputX')
    
    ##################tf.placeholder(tf.float32,name='InputX')
    Y=tf.placeholder(tf.float32, shape=(BATCH_SIZE,),name='InputY')
    ##################tf.placeholder(tf.float32,[90,1,1,1],name='InputY')
Num_Kernels_Con = 10
with tf.name_scope('Weights'):
#weight
    W1 = tf.Variable(tf.random_normal([1,5],stddev=0.1),name='Weights')
    W2 = tf.Variable(tf.random_normal([100,1],stddev=0.1),name='Weights')
    conv1_weights = tf.Variable(tf.random_normal([1, 2, NUM_CHANNELS, Num_Kernels_Con],
                          stddev=0.1,
                          dtype=np.float32),name='pesosConv1') 
    
    

with tf.name_scope('Biases'):
    #b1 = tf.Variable(tf.random_normal([1,5]),name='Biases')
    b1 = tf.Variable(tf.random_normal([100]),name='Biases')
    b2 = tf.Variable(tf.random_normal([1]),name='Biases')
    bconv = tf.Variable(tf.random_normal([Num_Kernels_Con]),name='Biases')
    
#Model-linear regression
def model(data):
    with tf.name_scope('Model'):
        c1 = tf.nn.conv2d(data,
                        conv1_weights,
                        strides=[1, 1, 1, 1],
                        padding='SAME')
        #conv1 = tf.nn.relu(tf.nn.bias_add(c1, bconv))
        conv1 = tf.nn.relu(tf.nn.bias_add(c1, bconv))
        pool1 = tf.nn.avg_pool(conv1, ksize=[1, 1, 3, 1], strides=[1, 1, 1, 1],
                         padding='SAME', name='pool1')
        
        reshape = tf.reshape(pool1, [BATCH_SIZE, -1])
        dim = reshape.get_shape()[1].value
        W = tf.Variable(tf.truncated_normal(shape=[dim,100],stddev=0.1),name='Weights')
        
        hidden = tf.add(tf.matmul(reshape,W),b1)
        activation = tf.add(tf.matmul(hidden, W2), b2, name='FUllConnected')
        
        
    return activation

Y_pred = model(X)
#Y_pred = model(train_data)
#Model-linear regression
#with tf.name_scope('Model'):
#    Y_pred = tf.sigmoid(tf.add(tf.mul(X, W), b, name='Y_Pred'))

#cost
with tf.name_scope('Loss'):
    #loss = tf.reduce_mean(tf.square(Y_pred - Y))
    loss = tf.reduce_mean(tf.square(Y_pred - train_labels))
#training algorithm
optimizer = tf.train.GradientDescentOptimizer(0.00001)
train = optimizer.minimize(loss)

#initializing the variables
init = tf.initialize_all_variables()

#starting the session session 
sess = tf.Session()
sess.run(init)
cost = tf.scalar_summary("loss",loss)

sess.run(init)
merged_summarry_op = tf.merge_all_summaries()
summary_writer = tf.train.SummaryWriter(train_dir,graph=tf.get_default_graph())
# training the line
epochs = 100000
#Plot
train_size = train_labels.shape[0]
#for step in range(int(epochs * train_size) // BATCH_SIZE):
for step in range(epochs):
    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)
    #offset = (step * BATCH_SIZE) % (train_size) #PEGA TODO O CONJUNTO DE DADOS PARA TREINO
    batch_data = train_data[offset:(offset + BATCH_SIZE), ...]
    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]
    #_, c, summary=sess.run([train, loss,merged_summarry_op], feed_dict={X: xs, Y: ys})
    feed_dict = {X: batch_data, Y: batch_labels}
    
    _, c, summary=sess.run([train, loss,merged_summarry_op], feed_dict)
    summary_writer.add_summary(summary,step)
  

#test 
#x_test=np.linspace(-np.pi,np.pi,200)
#x_test = x_test.reshape(200,1)
end = train_data.shape[0]
x_test = train_data[end -BATCH_SIZE:end, ...]
y_test=sess.run(Y_pred,feed_dict = {X:x_test})
y_real = train_labels[end -BATCH_SIZE:end]
#plt.plot(xs,ys,'o', x_test,y_test,"*")
# plt.scatter(y_real,y_test)
plt.show()

fig, ax = plt.subplots(1,1)
for step in range(train_data.shape[0]// BATCH_SIZE):
     offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)
     x_test = train_data[offset:(offset + BATCH_SIZE), ...]
     y_real = train_labels[offset:(offset + BATCH_SIZE)]
     y_test=sess.run(Y_pred,feed_dict = {X:x_test})
     
     ax.scatter(y_real,y_test)
     
     
sess.close()